
custom:
  spark-conf-auth: &spark-conf-auth {% include 'sparkconf.json.j2' %}

  spark-conf-basic: &spark-conf-basic
    <<: *spark-conf-auth
    spark.databricks.io.cache.enabled: true
    spark.databricks.delta.preview.enabled: true
    spark.databricks.delta.schema.autoMerge.enabled: true

  spark-conf-single: &spark-conf-single
    <<: *spark-conf-basic
    spark.databricks.cluster.profile: singleNode
    spark.master: local[*, 4]

  azure-attributes: &azure-attributes
    availability: ON_DEMAND_AZURE
    first_on_demand: 1
    spot_bid_max_price: -1

  basic-cluster-general: &basic-cluster-general
    spark_version: "11.3.x-scala2.12"
    spark_conf: *spark-conf-basic
    azure_attributes: *azure-attributes
    spark_env_vars:
      PYSPARK_PYTHON: /databricks/python3/bin/python3
    instance_pool_name: "Standard L4s instances runtime"


  basic-cluster-single: &basic-cluster-single
    spark_version: "11.3.x-scala2.12"
    spark_conf: *spark-conf-single
    azure_attributes: *azure-attributes
    spark_env_vars:
      PYSPARK_PYTHON: /databricks/python3/bin/python3
    instance_pool_name: "Standard L4s instances runtime"
    num_workers: 0


  common_task_setting: &common_task_setting
    libraries: {% include 'sparklibs.json' %}


  multi_task_job_settings: &multi_task_job_settings
    format: MULTI_TASK
    <<: *common_job_settings
    job_clusters:
      - job_cluster_key: StrongCluster1
        new_cluster:
          <<: *basic-cluster-general
          num_workers: 2
      - job_cluster_key: SmallCluster1
        new_cluster:
          <<: *basic-cluster-single
          num_workers: 1